---
title: "Textual Data"
author: "Jilly MacKay"
date: "17 April 2018"
output: 
  slidy_presentation:
      highlight: haddock
      theme: flatly
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy = TRUE, tidy.opts=list(blank=FALSE, width.cutoff=60))
library(tidyverse)
library(tidytext)
library(gutenbergr)
library(tm)
library(wordcloud)
```

## A Fundamental Assertion

All 'data science' does is condense *lots of data* into a *smaller bit of data*. 


For example:
```{r}
lots.of.data <- runif(100, min = 0, max = 100)
```

## A Fundamental Assertion

All 'data science' does is condense *lots of data* into a *smaller bit of data*. 


If I asked you to interpret that data, you would *not* say:

```{r, eval = FALSE}
print(lots.of.data)
```
## A Fundamental Assertion

All 'data science' does is condense *lots of data* into a *smaller bit of data*. 


If I asked you to interpret that data, you would *not* say:

```{r}
print(lots.of.data)
```


## Dealing with Lots of Data

To interpret *lots of data* you would instead likely do something like:

```{r}
mean(lots.of.data)
sd(lots.of.data)

```



## Dealing with Lots of Data

To interpret *lots of data* you would instead likely do something like:

```{r}
mean(lots.of.data)
sd(lots.of.data)

```

The rest is just quibbling over methodology (which is extremely fun to do!)
```{r}
median(lots.of.data)


```


## Textual Data

In the same way, if you have a large chunk of text, you do not 'analyse' it by saying ...

```{r, include = FALSE}
lotta.text <- gutenberg_download(31847)
lotta.text <- lotta.text %>%
  select (-gutenberg_id)

```

```{r}
print(lotta.text)
```


## Textual Data
There are three main ways to analyse textual data in R

* As corpora (`tm`)
* As tidytext (`tidytext`)
* Thematically (`RQDA`)


(*Lets use a simpler example than `lottatext`*)

```{r, tidy = FALSE}
data <-  c("", 
           "word1",  
           "word1 word2", 
           "word1 word2 word3",
           "word1 word2 word3 word4",
           "word1 word2 word3 word4 word5")
```

## The tm Package
The `tm` package is about 'text mining' and treats text as a 'corpus' (or many 'corpora').

 

The basic command is:
```{r}
tm.text <- Corpus(VectorSource(data))
```

Where `Corpus` reads in the data.
 
 
`VectorSource` tells `Corpus` to expect the source file to be in a vector format. 



## The tm Package
A corpus can be thought of as a document of documents. 
 

In this case, using the command `VectorSource` has each 'row' of the example data as a new document. 
 


If you have multiple big text files in a folder you can use `DirSource` as opposed to `VectorSource`

 
`DirSource` will read all the documents in the directory it's pointed to. 
 

Think of each document as an 'observation'. 


## Document Term Matrices vs Term Document Matrices
Tutorials often use these interchangeably, but there are times you want one, and times you want the other.

The key difference is that `DocumentTermMatrix` has documents listed in the first **column**
```{r}
dtm.data <- DocumentTermMatrix(tm.text)
head(as.matrix(dtm.data))
```

## Document Term Matrices vs Term Document Matrices
Versus the  `TermDocumentMatrix` has documents listed in the first **row**
```{r}
tdm.data <- TermDocumentMatrix(tm.text)
head(as.matrix(tdm.data))
```


## Using Document Term Matrices for Frequency
You can explore datasets quickly using these matrices. 
```{r}
# TermDocumentMatrix
tdm.m <- as.matrix(tdm.data)
tdm.v <- sort(rowSums(tdm.m), decreasing = TRUE)
tdm.d <- data.frame(word = names(tdm.v), freq = tdm.v)
head(tdm.d, 10)

```

## Exploring Frequency with tm Commands
There are a few interesting commands in the `tm` package which are frequently found in online tutorials (there are caveats to how they work though) 

 
```{r}
#Find the top 3 terms
findFreqTerms(tdm.data, 3)

#Find correlations above 0 with Word 1 (Pearson correlations, 0 is lower limit)
findAssocs(tdm.data, "word1", 0)
```

## Word Clouds
People love to visualise text data with word clouds. `tm` works very well with `wordcloud` to do this (`wordcloud` expects data in Corpus format)

```{r}
wordle <- wordcloud(tm.text, scale = c(5,0.5), max.words = 10, random.order = FALSE, random.color = FALSE, rot.per = 0, use.r.layout = FALSE)
wordle
```

## tm versus tidytext
The `tm` package is very good at word frequency, and very little else. 

My preferred alternative is `tidytext`.

Corpora are documents as you or I would read them. `tidytext` converts text into tidy data. 

```{r}
data_tb <- tibble(text = data)

#This command unnests all the words so every word is on a new row sequentially
data_tb_un <- data_tb %>%
    unnest_tokens(word, text)
```

## Using tidytext for Frequency Analysis

```{r}
data_freq <- data_tb_un %>%
  count (word, sort =TRUE)
data_freq
```

## Using tidytext for Frequency Analysis
```{r}
data_freq <- data_tb_un %>%
  count (word, sort =TRUE)
data_freq
```

Like all tidyverse stuff, this can be built up in oniony layers

```{r}
data_freq <- data_tb_un %>%
  count(word, sort = TRUE) %>%
  top_n (3)
data_freq

```

## Using tidytext for Frequency Analysis
This can also be visualised like so:
```{r}
data_freq <- data_tb_un %>%
  count(word, sort = TRUE) %>%
  top_n (3) %>%
  ungroup() %>%
  mutate(text_order = nrow(.):1)


ggplot (data = data_freq, aes(reorder(word, text_order), n)) +
  geom_bar (stat = "identity") +
  labs(x = "Word", y= "Frequency in 'data'") +
  coord_flip()+
  theme_classic()
```

## Word Associations via n-grams
Instead of  a tibble where each word is on a different row . . . 

Create a tibble where each row is a sequential pair of words (bigrams). 

This creates repetition, in the first two rows of the new tibble `data_eng` there is an overlap, row1-column2's 'word1' is the same as row2-column1's 'word1'

```{r}

data_eng <- data_tb %>%
    unnest_tokens(bigram, text, token = "ngrams", n = 2)

data_eng
```

## Word Associations via n-grams

We can count the bigrams in the same way as before

```{r}
bigramcount <- data_eng %>%
  count (bigram, sort = TRUE)
bigramcount
```

## Word Associations via n-grams

We can count the bigrams in the same was as before

```{r}
ggplot (data = bigramcount, aes(x = (reorder(bigram,n)), y = n)) +
  geom_bar (stat = "identity") +
  labs(x = "Bigram", y= "Frequency in 'data'") +
  coord_flip()+
  theme_light()

```


## RQDA
`RQDA` is a package which allows R to 'code' free-text data. 'Coding' in this context, means 'categorising'. 

R cannot thematically analyse text **for** you. It can only organise your categories. 

`RQDA` is available [here - http://rqda.r-forge.r-project.org/](http://rqda.r-forge.r-project.org/). There are a few intermediary steps to installing `RQDA`, as it requires GTK+ to work. 

## RQDA

We're going to use a document which is freely available from the Gutenberg Project, (Dog Stories from the Spectator)[http://www.gutenberg.org/ebooks/31847].

```{r, eval = FALSE}
#To open RQDA
RQDA()
```

A new window will open ...


## RQDA


![Opening RQDA](Images\cap01.png)


## RQDA


![Create a new project](Images\cap02.png)

## RQDA


![In the 'files' tab, you can associate text with the project](Images\cap03.png)

## RQDA


![Specify your text files](Images\cap04.png)

## RQDA


![Now in qualitative analysis, you will read through your text files (many, many times)](Images\cap05.png)

## RQDA


![Next is the 'code' tab, where you can start to note down the themes that arise in your text](Images\cap06.png)

## RQDA


![You can 'add' codes (or themes) apriori, like so](Images\cap07.png)

## RQDA


![And you'll end up with multiple codes like so.](Images\cap08.png)


## RQDA


![The 'meat' of qual analysis is to find the interesting information and 'file' it to a code.](Images\cap09.png)


## RQDA


![Here we'll add a new code, 'Dog Wisdom'](Images\cap10.png)

## RQDA


![Once the code is created, highlight the text, ensure the correct code is highlighted, and click 'mark'](Images\cap11.png)


## RQDA

You can then pull up all the text within a given code and code within or review that text. 

## Free Text Three Ways

At the end of the day, all three methods have their strengths. 

* Corpora are traditional methods of exploring frequency, and can be explored with time.
* Tidytext is a highly repeatable and robust way of dealing with text data
* Qualitative analysis is really the only way to explore the linkages of themes. 

Happy text mining!

## Fin

*FIN*